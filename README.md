## ST-MoE - Pytorch (wip)

Implementation of <a href="https://arxiv.org/abs/2202.08906">ST-MoE</a>, the final incarnation of MoE after years of research at Brain, in Pytorch. <a href="https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py">Official Jax Implementation</a>, but in Mesh Tensorflow.

## Citations

```bibtex
@inproceedings{Zoph2022STMoEDS,
    title   = {ST-MoE: Designing Stable and Transferable Sparse Expert Models},
    author  = {Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and Jeff Dean and Noam M. Shazeer and William Fedus},
    year    = {2022}
}
```
